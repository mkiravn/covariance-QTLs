---
title: "Gradient descent to estimate w"
output:
  html_document:
    df_print: paged
---

Loading some functions:

```{r}
library(mvtnorm)
library(MASS)
library(tidyverse)
library(Matrix)

greml <- function( y, G, constrain=F, quantnorm=F ){
  if( quantnorm ){
    ranks <- rank(y)
    quantiles <- qnorm(ranks/(length(ranks) + 1))
    y <- quantiles
  }
  y <- scale(y)
  G <- scale(G)
  N <- nrow(G)
  svdG <- svd(G)
  out <- eigen_lmm(yprime=t(svdG$u) %*% y, Lam.K=(svdG$d/sqrt(N)+1e-12)^2, constrained=constrain)
  sig2g <- out$sig2g
  sig2e <- out$sig2e
  list( h2=sig2g/(sig2g+sig2e), sig2g=sig2g, sig2e=sig2e )
}

scale01 <- function (x)
{
  x <- x - min(x, na.rm = T) + 1e-8
  x <- x/max(x, na.rm = T) - 1e-8
  x
}


eigen_lmm <- function (yprime, Lam.K, constrained = TRUE) {
  if (constrained) {
    delta <-
      optimise(
        eigen_lmm_obj,
        c(0, 1e5),
        maximum = TRUE,
        Lam.K = Lam.K,
        yprime = yprime
      )$maximum
  } else {
    ### these parameters correspond to h^2>0, with h^2>1 for delta<0
    opt_lims_plus <- c(-min(Lam.K), 1e5)
    opt_plus <-
      optimise(
        eigen_lmm_obj,
        opt_lims_plus,
        maximum = TRUE,
        Lam.K = Lam.K,
        yprime = yprime
      )
    ### these parameters correspond to h^2<0
    opt_lims_minus <- c(-1000, -max(Lam.K))
    opt_minus <-
      optimise(
        eigen_lmm_obj,
        opt_lims_minus,
        maximum = TRUE,
        Lam.K = Lam.K,
        yprime = yprime
      )
    delta <-
      ifelse(opt_plus$objective > opt_minus$objective,
             opt_plus$maximum,
             opt_minus$maximum)
  }
  sig2g <- mean(yprime ^ 2 / (Lam.K + delta))
  sig2e <- delta * sig2g
  list(h2 = sig2g / (sig2g + sig2e),
       sig2g = sig2g,
       sig2e = sig2e)
}


eigen_lmm_obj <- function(delta, yprime, Lam.K){
  denominator <- Lam.K + delta
  sig2g <- mean(yprime^2/denominator)
  sum(dnorm( yprime, mean=0, sd=sqrt(sig2g*denominator), log=TRUE ))}


simulate_genotypes <- function(N,L){
  ### function to simulate genotypes
  ps <- rbeta(n = L,4,4)
  G <- matrix(NA,nrow=N,ncol=L)
  for (i in c(1:L)){
    gen <- rbinom(N, size = 2, prob = ps[i])
    G[,i] <- gen
  }
  return(G)
}

give_disease_risk <- function(nodes,f=mean,threshold=5){
  # returns disease risk/severity 
  # risk given by some function of the nodes
  risk <- f(nodes)
  return(risk)
}

# a more funky disease function
mdd_risk <- function(v,threshold=1,n=3){
  # linear combination of node activation - this is just an example
  return(sum(v > threshold)>n)
}
```


Now we simulate the symptoms:

```{r}
h20 <- 0.5
p <- 2
w <- 0.5
Sigma_i <- w * matrix(1,p,p) + (1-w) * diag(1,p,p)


### Defining parameters
N <- 500 # number of individuals
l <- 100 # number of loci - don't need many for GREML
reps <- 10 # number of replicates
p <- 2 # number of symptoms
her <- 0 # heritability of symptoms

# admin
J <- matrix(1,p,p)
rows <- data.frame()

hercov <- 1

# simulate a genotype matrix
G <- simulate_genotypes(N = N,L = l)
G <- scale(G)
# genetic effects on covariance
beta_cov <- matrix(mvrnorm(n=l,mu = rep(0,1),Sigma = diag(1)),nrow=l) %*% diag(1,1,1) * sqrt(hercov / l)
e_cov <- matrix(mvrnorm(n=N,mu = rep(0,1),Sigma = diag(1)),nrow=N) %*% diag(1,1,1) * sqrt(1-hercov)
J_weights <- G %*% beta_cov + e_cov
J_weights <- scale01(J_weights)
# genetic effects on symtpoms
beta <- matrix(mvrnorm(n=l,mu = rep(0,p),Sigma = diag(5,p,p)),nrow=l) %*% diag(1,p,p) * sqrt(her / l)
e <- matrix(mvrnorm(n=N,mu = rep(0,p),Sigma = diag(5,p,p)),nrow=N) * sqrt(1-her)
# symptom matrix
X <- matrix(0,nrow=N,ncol=p)
# Give each person a covariance matrix and draw symptoms
for (ind in c(1:N)){
  # get weight
  ind_J_weight <- J_weights[ind]
  Sig_ind <- ind_J_weight * J + (1-ind_J_weight) * diag(1,p,p)
  beta_transformed <- t(chol(Sig_ind)) %*% t(beta) %>% t() # check this
  e_transformed <- t(chol(Sig_ind)) %*% e[ind,] %>% t() # check this
  X[ind,] <- G[ind,] %*% beta_transformed + e_transformed
}
# give the disease
Y <- apply(X,1,mdd_risk,threshold=1,n=1)
prev <- sum(Y)/N
# estimating heritabilities
h2J <- greml(J_weights,G)$h2
h2IP <- greml(X[,1] * X[,2],G)$h2
h2Y <- greml(Y,G)$h2
cor(X)

df <- data.frame(X) %>%
  mutate(IP12=((X1-mean(X1))* (X2-mean(X2)))/sqrt(var(X1)*var(X2)))

w <- df$IP12
```
Let's write some functions to get the gradient with respect to w. I will separate out the gradient into two parts. The first will deal with the information from symptoms.


$$
\nabla_{w_j} \ell^{cov} 
= dp/( 1-w_j ) (  1 - p ) + 1/( 1-w_j )^2 (X_{j,}^T (- I  + J r_j ) X_{j,})
$$
where $p$ is the number of symptoms, $d = \frac{1}{(1-w_j)/w_j+p}$, and $r_j= 1 -  2 ( d p + d ) + d^2 p^2 - d^2 p$


```{r}
get_d <- function(wj,p){
  return(1/((1-wj)/wj+p))
}

get_r <- function(d,p){
  return(1 -  2 *  ( d * p + d ) + d^2 * p^2 - d^2* p)
}

grad_cov <- function(X,j,w){
  
  wj <- w[j]

  p <- dim(X)[2]
  dj <- get_d(wj,p)
  rj <- get_r(dj,p)

  g <- dj * p / ((1 - wj) * (1 - p)) + 1 / (1 - wj) ^ 2 *
    t(matrix(X[j, ])) %*% (-diag(1, p, p) + rj * matrix(1, p, p)) %*% matrix(X[j, ])

  return(g)
}
```




```{r}
gc <- c()

j <- 12
for (weight in seq(-1,1,0.1)){
  wj <- weight

  p <- dim(X)[2]
  dj <- get_d(wj,p)
  rj <- get_r(dj,p)

  g <- dj * p / ((1 - wj) * (1 - p)) + 1 / (1 - wj) ^ 2 *
    t(matrix(X[j, ])) %*% (-diag(1, p, p) + rj * matrix(1, p, p)) %*% matrix(X[j, ])
  
  gc <- c(gc,g)
}

df$IP12[j]
J_weights[j]
X[j,]

plot(seq(-1,1,0.1),gc,xlim = c(-3,3))
abline(v=df$IP12[j],col="orchid",lty=2) # should be the maximum since we haven't factored in kinship
abline(v=J_weights[j],col="cyan2",lty=2) # the true weight
```



The second part of the gradient includes information from the kinship matrix - it constrains the vector of $w$ based on relatedness.

$$
\nabla_{w_j} \ell^{rel} 
= 2 (h^2_w K + (1-h^2_w)I)^{-1})_{j,} w
$$
This step is more time intensive because we need the inverse of the $w$ covariance matrix. We can lighten it up by noting that 

\begin{align}
(h^2_w K + (1-h^2_w)I)^{-1} &= (h^2_wQ \Lambda Q^T + (1-h^2_w)I)^{-1} \\&= Q( h^2_w\Lambda + (1-h^2_w)I)^{-1}Q^T
\end{align}


```{r}
# precompute some stuff
K <- cor(t(G))
eigenK <- eigen(K)
Q <- eigenK$vectors
Lam <- eigenK$values

grad_rel <- function(h2,j,w,K){
  
  2 * chol2inv(h2 * K + (1-h2) * diag(1,dim(K)[1]))[j,] %*% matrix(w)
  
}

grad_rel(h2=0.5,j=3,w,K)
```


--- not true below

So
\begin{align}
-2( h^2_w K + (1-h^2_w) I )_j^{-1} w &= [-2Q( h^2_w\Lambda + (1-h^2_w)I)_j^{-1}Q^Tw]_j \\
  &= [-2Q(1/(h^2_w\Lambda_j + (1-h^2_w))I)Q^Tw]_j \\
  &= -2(1/(h^2_w\Lambda_j + (1-h^2_w)))[QQ^Tw]_j
\end{align}

```{r}
# precompute some stuff
K <- cor(t(G))
eigenK <- eigen(K)
Q <- eigenK$vectors
Lam <- eigenK$values

QQt <- Q %*% t(Q) # this is the identity

grad_rel <- function(h2,j,w,QQt,Lam){
  
  Qw <- QQt %*% matrix(w)
  return(-2 * (1/(h2 * Lam[j]  + (1-h2)))  * Qw[j])
  
}

grad_rel(h2=0.5,j=c(1:N),w,QQt,Lam)
```



Now the algorithm


```{r}
maxit <- 100
stepsize <- 0.001

# step 1: initialise w and h2
w <- df$IP12
w <- J_weights
h2 <- greml(w,G)$h2
print(h2)
print(paste("squared norm of true minus estimated weights:", sum(J_weights-w)^2))

# step 2: iterate
for (it in c(1:maxit)){
  ### take each element w and update based on gradient

  for (i in c(1:length(w))){

    gc <- grad_cov(X,i,w)
    gr <- grad_rel(h2,i,w,K)

    gradw <- gc + gr
    
    new_wj <- w[i] - stepsize * gradw
    w[i] <- new_wj
  }
  
  print(w[1:10])
  
  print(paste("squared norm of true minus estimated weights:", mean((J_weights-w)^2))) # should be decreasing
  
  h2 <- greml(w,G)$h2
  
  print(paste("h2 estimate:",h2)) # should be approaching 1
  
}

print(head(w))

```


Problems: 

* We appear to be getting further from the true weights, not closer.
* Entries in $w$ are unconstrained, so they now go over 1 and below -1, which should not be allowed. However, scaling $w$ after every iteration doesn't fix this.